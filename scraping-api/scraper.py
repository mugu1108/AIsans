"""
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯
GASã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã®ç§»æ¤ + ä¼æ¥­åä¸€è‡´ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½
"""

import re
import asyncio
import logging
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass
from typing import Optional

import httpx
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


# ====================================
# å®šæ•°
# ====================================

# é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆæ±‚äººã‚µã‚¤ãƒˆã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€SNSç­‰ï¼‰
EXCLUDE_DOMAINS = [
    # æ±‚äººã‚µã‚¤ãƒˆ
    'indeed.com', 'indeed.jp', 'mynavi.jp', 'rikunabi.com', 'doda.jp',
    'en-japan.com', 'baitoru.com', 'careerconnection.jp', 'jobchange.jp', 'hatarako.net',
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢
    'yahoo.co.jp', 'news.yahoo.co.jp', 'nikkei.com', 'asahi.com', 'yomiuri.co.jp',
    'mainichi.jp', 'sankei.com',
    # SNS
    'facebook.com', 'twitter.com', 'x.com', 'instagram.com',
    'youtube.com', 'tiktok.com', 'linkedin.com',
    # ç™¾ç§‘äº‹å…¸
    'wikipedia.org', 'ja.wikipedia.org',
    # ECãƒ»å¤§æ‰‹
    'google.com', 'amazon.co.jp', 'rakuten.co.jp',
    # ä¼æ¥­æƒ…å ±ãƒ»å£ã‚³ãƒŸã‚µã‚¤ãƒˆ
    'bizmap.jp', 'baseconnect.in', 'wantedly.com', 'vorkers.com', 'openwork.jp',
    # åœ°å›³ãƒ»ãƒŠãƒ“ãƒ»æ–½è¨­æ¤œç´¢
    'navitime.co.jp', 'mapion.co.jp', 'mapfan.com', 'ekiten.jp',
    'hotpepper.jp', 'tabelog.com', 'gnavi.co.jp', 'retty.me',
    # è»¢è·ãƒ»ã‚­ãƒ£ãƒªã‚¢ç³»ãƒãƒ¼ã‚¿ãƒ«
    'career-x.co.jp', 'type.jp', 'green-japan.com', 'mid-tenshoku.com',
    # ãƒ–ãƒ­ã‚°ãƒ»æŠ€è¡“ç³»
    'note.com', 'qiita.com', 'zenn.dev', 'hateblo.jp', 'ameblo.jp',
    # ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹
    'prtimes.jp', 'atpress.ne.jp',
    # ä¼æ¥­ãƒªã‚¹ãƒˆãƒ»ã¾ã¨ã‚
    'geekly.co.jp', 'imitsu.jp', 'houjin.jp',
    'factoring.southagency.co.jp', 'mics.city.shinagawa.tokyo.jp',
    'best100.v-tsushin.jp', 'isms.jp', 'itnabi.com',
    'appstars.io', 'ikesai.com', 'rekaizen.com', 'careerforum.net',
    'startupclass.co.jp', 'herp.careers', 'readycrew.jp', 'ai-taiwan.com.tw',
    'utilly.ne.jp', 'hatarakigai.info', 'officenomikata.jp', 'cheercareer.jp'
]

# ãŠå•ã„åˆã‚ã›URLæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
CONTACT_KEYWORDS = [
    'contact', 'inquiry', 'enquiry', 'toiawase', 'otoiawase',
    'ãŠå•ã„åˆã‚ã›', 'ãŠå•åˆã›', 'ãŠå•åˆã‚ã›', 'ãŠã¨ã„ã‚ã‚ã›',
    'form', 'mail', 'support'
]

# ã‚ˆãã‚ã‚‹ãŠå•ã„åˆã‚ã›URLãƒ‘ã‚¿ãƒ¼ãƒ³
COMMON_CONTACT_PATHS = [
    'contact/',
    'contact',
    'inquiry/',
    'contact.html',
    'toiawase/',
    'otoiawase/',
    'form/',
    'contact-us/',
    'contactus/',
    'mail/',
    'support/',
    'info/',
    'ask/',
    'inquiry.html',
    'contact/index.html',
]

# æ³•äººæ ¼é™¤å»ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¼æ¥­åæ­£è¦åŒ–ç”¨ï¼‰
CORPORATE_SUFFIXES = [
    'æ ªå¼ä¼šç¤¾', '(æ ª)', 'ï¼ˆæ ªï¼‰',
    'æœ‰é™ä¼šç¤¾', '(æœ‰)', 'ï¼ˆæœ‰ï¼‰',
    'åˆåŒä¼šç¤¾', 'åˆè³‡ä¼šç¤¾', 'åˆåä¼šç¤¾',
    'ä¸€èˆ¬ç¤¾å›£æ³•äºº', 'ä¸€èˆ¬è²¡å›£æ³•äºº', 'å…¬ç›Šç¤¾å›£æ³•äºº', 'å…¬ç›Šè²¡å›£æ³•äºº',
    'ç‰¹å®šéå–¶åˆ©æ´»å‹•æ³•äºº', 'NPOæ³•äºº',
    'Inc.', 'Co.,Ltd.', 'Ltd.', 'Corp.', 'LLC', 'LLP',
    'Corporation', 'Company', 'Co.'
]

# HTTPãƒ˜ãƒƒãƒ€ãƒ¼
HTTP_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
}

HTTP_TIMEOUT = 10.0  # ç§’
MAX_CONCURRENT = 10  # åŒæ™‚æ¥ç¶šæ•°


# ====================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ====================================

@dataclass
class ScrapeResult:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ"""
    company_name: str
    base_url: str
    contact_url: str
    phone: str
    domain: str
    error: str


# ====================================
# URLæ“ä½œé–¢æ•°
# ====================================

def normalize_to_top_page(url: str) -> str:
    """URLã‚’ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«æ­£è¦åŒ–"""
    try:
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}/"
    except Exception:
        return url


def extract_domain(url: str) -> str:
    """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º"""
    try:
        parsed = urlparse(url)
        return parsed.netloc
    except Exception:
        return url


def is_same_domain(url1: str, url2: str) -> bool:
    """åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯"""
    return extract_domain(url1) == extract_domain(url2)


def is_excluded_domain(domain: str) -> bool:
    """é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯"""
    domain_lower = domain.lower()
    return any(excluded in domain_lower for excluded in EXCLUDE_DOMAINS)


def resolve_url(base_url: str, relative_url: str) -> str:
    """ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›"""
    if relative_url.startswith('http'):
        return relative_url
    return urljoin(base_url, relative_url)


# ====================================
# ä¼æ¥­åæ­£è¦åŒ–ãƒ»ä¸€è‡´ãƒã‚§ãƒƒã‚¯
# ====================================

def normalize_company_name(name: str) -> str:
    """
    ä¼æ¥­åã‚’æ­£è¦åŒ–
    - å°æ–‡å­—åŒ–
    - æ³•äººæ ¼é™¤å»
    - ç©ºç™½ãƒ»è¨˜å·é™¤å»
    """
    normalized = name.lower()

    # æ³•äººæ ¼é™¤å»
    for suffix in CORPORATE_SUFFIXES:
        normalized = normalized.replace(suffix.lower(), '')

    # ç©ºç™½ãƒ»è¨˜å·é™¤å»
    normalized = re.sub(r'[\s\u3000ãƒ»\-\(\)ï¼ˆï¼‰ã€ã€‘ã€Œã€ã€ã€\[\]]+', '', normalized)

    return normalized.strip()


def check_company_match(company_name: str, html: str) -> bool:
    """
    ä¼æ¥­åã¨ãƒšãƒ¼ã‚¸å†…å®¹ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯

    Returns:
        True: ä¸€è‡´ï¼ˆOKï¼‰
        False: ä¸ä¸€è‡´ï¼ˆcompany_mismatchï¼‰
    """
    normalized_name = normalize_company_name(company_name)

    # æ­£è¦åŒ–å¾Œã®ä¼æ¥­åãŒ2æ–‡å­—æœªæº€ã®å ´åˆã€ãƒã‚§ãƒƒã‚¯ã‚¹ã‚­ãƒƒãƒ—
    if len(normalized_name) < 2:
        return True

    soup = BeautifulSoup(html, 'lxml')

    # titleã‚¿ã‚°å–å¾—
    title = ''
    title_tag = soup.find('title')
    if title_tag:
        title = normalize_company_name(title_tag.get_text())

    # og:site_nameå–å¾—
    og_site_name = ''
    og_tag = soup.find('meta', property='og:site_name')
    if og_tag and og_tag.get('content'):
        og_site_name = normalize_company_name(og_tag['content'])

    # ä¸€è‡´åˆ¤å®š
    # name ãŒ title ã¾ãŸã¯ og ã«å«ã¾ã‚Œã¦ã„ã‚‹
    if normalized_name in title:
        return True
    if normalized_name in og_site_name:
        return True

    # titleï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰ãŒ name ã«å«ã¾ã‚Œã¦ã„ã‚‹
    if len(title) >= 2 and title in normalized_name:
        return True

    # ogï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰ãŒ name ã«å«ã¾ã‚Œã¦ã„ã‚‹
    if len(og_site_name) >= 2 and og_site_name in normalized_name:
        return True

    # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ãƒ»ä¼šç¤¾æ¦‚è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ä¼æ¥­åã‚’æ¤œç´¢
    # ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¿ã‚¤ãƒˆãƒ«ã«ä¼æ¥­åãŒãªãã¦ã‚‚æœ¬æ–‡ã«ã‚ã‚Œã°ä¸€è‡´ã¨ã¿ãªã™
    for selector in ['header', 'footer', '.company', '#company', '.about', '#about', '.corp', '#corp']:
        elements = soup.select(selector)
        for elem in elements:
            elem_text = normalize_company_name(elem.get_text())
            if normalized_name in elem_text:
                return True

    # ä¼æ¥­åã®ä¸»è¦éƒ¨åˆ†ï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰ãŒãƒšãƒ¼ã‚¸å…¨ä½“ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if len(normalized_name) >= 3:
        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»ã‚¹ã‚¿ã‚¤ãƒ«é™¤å»ï¼‰
        for script in soup(['script', 'style', 'noscript']):
            script.decompose()
        body_text = normalize_company_name(soup.get_text())

        # ä¼æ¥­åãŒãƒšãƒ¼ã‚¸æœ¬æ–‡ã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
        if normalized_name in body_text:
            return True

    return False


# ====================================
# é›»è©±ç•ªå·å‡¦ç†
# ====================================

def is_valid_phone_number(phone: str) -> bool:
    """é›»è©±ç•ªå·ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    digits = re.sub(r'\D', '', phone)
    if len(digits) < 10 or len(digits) > 11:
        return False
    if not digits.startswith('0'):
        return False
    if '0000' in digits:
        return False
    return True


def format_phone_number(phone: str) -> str:
    """é›»è©±ç•ªå·ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    digits = re.sub(r'\D', '', phone)

    # 03å§‹ã¾ã‚Š10æ¡
    if len(digits) == 10 and digits[:2] == '03':
        return f"{digits[:2]}-{digits[2:6]}-{digits[6:]}"

    # æºå¸¯ç•ªå·11æ¡
    if len(digits) == 11 and digits[:2] in ('09', '08', '07'):
        return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"

    # 0120å§‹ã¾ã‚Š10æ¡
    if len(digits) == 10 and digits[:4] == '0120':
        return f"{digits[:4]}-{digits[4:7]}-{digits[7:]}"

    # ãƒã‚¤ãƒ•ãƒ³ãŒæ—¢ã«ã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾
    if '-' in phone:
        return phone

    # ãã®ä»–10æ¡
    if len(digits) == 10:
        return f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"

    # ãã®ä»–11æ¡
    return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"


def extract_phone_from_html(html: str) -> str:
    """HTMLã‹ã‚‰é›»è©±ç•ªå·ã‚’æŠ½å‡º"""

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: tel:ãƒªãƒ³ã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰
    tel_pattern = r'href=["\']tel:([0-9\-]+)["\']'
    tel_matches = re.findall(tel_pattern, html)
    for match in tel_matches:
        phone = re.sub(r'[^\d\-]', '', match).replace('--', '-')
        if is_valid_phone_number(phone):
            return format_phone_number(phone)

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ©ãƒ™ãƒ«ä»˜ãé›»è©±ç•ªå·
    labeled_pattern = r'(?:TEL|Tel|tel|é›»è©±|é›»è©±ç•ªå·|â˜|ğŸ“|â„¡|ä»£è¡¨)[:\sï¼š]*?\(?0\d{1,4}\)?[-\s\.\-]?\d{1,4}[-\s\.\-]?\d{3,4}'
    labeled_matches = re.findall(labeled_pattern, html)
    for match in labeled_matches:
        phone = re.sub(r'[^\d\-]', '', match).replace('--', '-')
        if is_valid_phone_number(phone):
            return format_phone_number(phone)

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ•°å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿
    digit_pattern = r'\b0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{3,4}\b'
    digit_matches = re.findall(digit_pattern, html)
    for match in digit_matches:
        phone = re.sub(r'[^\d\-]', '', match).replace('--', '-')
        if is_valid_phone_number(phone):
            return format_phone_number(phone)

    return ''


# ====================================
# ãŠå•ã„åˆã‚ã›URLæŠ½å‡º
# ====================================

def calculate_contact_score(href: str, text: str) -> int:
    """ãŠå•ã„åˆã‚ã›URLã®ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    score = 0
    href_lower = href.lower()
    text_lower = text.lower()

    if 'contact' in href_lower:
        score += 10
    if 'inquiry' in href_lower:
        score += 10
    if 'toiawase' in href_lower:
        score += 10
    if 'ãŠå•ã„åˆã‚ã›' in text_lower:
        score += 8
    if 'ãŠå•åˆã›' in text_lower:
        score += 8
    if 'form' in href_lower:
        score += 5

    # ãƒ‘ã‚¹ã®æ·±ã•
    path_depth = href.count('/')
    score += max(0, 5 - path_depth)

    return score


def extract_contact_from_html(html: str, base_url: str) -> str:
    """HTMLã‹ã‚‰ãŠå•ã„åˆã‚ã›URLã‚’æŠ½å‡º"""
    soup = BeautifulSoup(html, 'lxml')
    candidates = []

    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        text = a_tag.get_text(strip=True).lower()
        href_lower = href.lower()

        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        if href_lower.startswith('mailto:'):
            continue
        if href_lower.startswith('javascript:'):
            continue
        if href_lower.startswith('tel:'):
            continue

        # #ã§å§‹ã¾ã‚‹ãƒªãƒ³ã‚¯ã¯ #contact ã®ã¿è¨±å¯
        if href.startswith('#'):
            if href_lower != '#contact':
                continue

        # å¤–éƒ¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®ãƒªãƒ³ã‚¯ã¯é™¤å¤–
        if href_lower.startswith('http') and not is_same_domain(base_url, href):
            continue

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
        for keyword in CONTACT_KEYWORDS:
            if keyword in href_lower or keyword in text:
                if href_lower == '#contact':
                    full_url = base_url + '#contact'
                else:
                    full_url = resolve_url(base_url, href)
                score = calculate_contact_score(href, text)
                candidates.append({'url': full_url, 'score': score})
                break

    if candidates:
        candidates.sort(key=lambda x: x['score'], reverse=True)
        return candidates[0]['url']

    return ''


# ====================================
# éåŒæœŸHTTPå–å¾—
# ====================================

async def fetch_with_retry(
    client: httpx.AsyncClient,
    url: str,
    max_retries: int = 1
) -> Optional[str]:
    """ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§éåŒæœŸHTTPå–å¾—"""
    for attempt in range(max_retries + 1):
        try:
            response = await client.get(
                url,
                headers=HTTP_HEADERS,
                timeout=HTTP_TIMEOUT,
                follow_redirects=True
            )
            if response.status_code == 200:
                return response.text
            else:
                logger.debug(f"HTTP {response.status_code}: {url}")
        except httpx.TimeoutException:
            logger.debug(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
        except Exception as e:
            logger.debug(f"HTTPå–å¾—ã‚¨ãƒ©ãƒ¼: {url} - {type(e).__name__}")
        if attempt < max_retries:
            await asyncio.sleep(0.3)
    return None


async def try_common_contact_urls(
    client: httpx.AsyncClient,
    base_url: str
) -> str:
    """ã‚ˆãã‚ã‚‹ãŠå•ã„åˆã‚ã›URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™"""
    for path in COMMON_CONTACT_PATHS:
        test_url = base_url + path
        html = await fetch_with_retry(client, test_url, max_retries=0)
        if html:
            html_lower = html.lower()
            if '<form' in html_lower or 'ãŠå•ã„åˆã‚ã›' in html_lower or 'contact' in html_lower:
                return test_url
    return ''


# ====================================
# å˜ä¸€ä¼æ¥­ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# ====================================

async def scrape_company(
    client: httpx.AsyncClient,
    company_name: str,
    url: str
) -> ScrapeResult:
    """å˜ä¸€ä¼æ¥­ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    base_url = normalize_to_top_page(url)
    domain = extract_domain(base_url)
    contact_url = ''
    phone = ''
    error = ''

    logger.debug(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {company_name} ({base_url})")

    # STEP 1: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸å–å¾—
    top_page_html = await fetch_with_retry(client, base_url, max_retries=1)

    if not top_page_html:
        logger.warning(f"ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {company_name} ({base_url})")
        return ScrapeResult(
            company_name=company_name,
            base_url=base_url,
            contact_url='',
            phone='',
            domain=domain,
            error='top_page_failed'
        )

    # STEP 2: ä¼æ¥­åä¸€è‡´ãƒã‚§ãƒƒã‚¯
    if not check_company_match(company_name, top_page_html):
        logger.warning(f"ä¼æ¥­åä¸ä¸€è‡´: {company_name} ({base_url})")
        return ScrapeResult(
            company_name=company_name,
            base_url=base_url,
            contact_url='',
            phone='',
            domain=domain,
            error='company_mismatch'
        )

    # STEP 3: ãŠå•ã„åˆã‚ã›URLæŠ½å‡º
    contact_url = extract_contact_from_html(top_page_html, base_url)

    # STEP 4: é›»è©±ç•ªå·æŠ½å‡ºï¼ˆãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
    phone = extract_phone_from_html(top_page_html)

    # STEP 5: ãŠå•ã„åˆã‚ã›ãƒšãƒ¼ã‚¸ã‹ã‚‰é›»è©±ç•ªå·å–å¾—
    if contact_url and not phone and '#' not in contact_url:
        contact_html = await fetch_with_retry(client, contact_url, max_retries=1)
        if contact_html:
            phone = extract_phone_from_html(contact_html)

    # STEP 6: ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™ï¼ˆãŠå•ã„åˆã‚ã›URLãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
    if not contact_url:
        contact_url = await try_common_contact_urls(client, base_url)

    # STEP 7: ä¼šç¤¾æ¦‚è¦ã‹ã‚‰é›»è©±ç•ªå·å–å¾—
    if not phone:
        about_urls = [base_url + 'company/', base_url + 'about/']
        for about_url in about_urls:
            about_html = await fetch_with_retry(client, about_url, max_retries=1)
            if about_html:
                phone = extract_phone_from_html(about_html)
                if phone:
                    break

    result = ScrapeResult(
        company_name=company_name,
        base_url=base_url,
        contact_url=contact_url,
        phone=phone,
        domain=domain,
        error=error
    )

    if contact_url or phone:
        logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {company_name} (contact: {bool(contact_url)}, phone: {bool(phone)})")
    else:
        logger.warning(f"é€£çµ¡å…ˆæœªæ¤œå‡º: {company_name} ({base_url})")

    return result


# ====================================
# ãƒãƒƒãƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
# ====================================

async def scrape_companies(
    companies: list[dict]
) -> list[ScrapeResult]:
    """
    è¤‡æ•°ä¼æ¥­ã‚’ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

    Args:
        companies: [{"company_name": "...", "url": "..."}, ...]

    Returns:
        ScrapeResult ã®ãƒªã‚¹ãƒˆ
    """
    results = []
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    async def scrape_with_semaphore(client: httpx.AsyncClient, company: dict) -> ScrapeResult:
        async with semaphore:
            result = await scrape_company(
                client,
                company.get('company_name', ''),
                company.get('url', '')
            )
            await asyncio.sleep(0.2)  # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«
            return result

    # SSLæ¤œè¨¼ç„¡åŠ¹ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
    async with httpx.AsyncClient(verify=False) as client:
        tasks = [
            scrape_with_semaphore(client, company)
            for company in companies
        ]
        results = await asyncio.gather(*tasks)

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
    results_list = list(results)
    total = len(results_list)
    success = sum(1 for r in results_list if r.contact_url or r.phone)
    top_failed = sum(1 for r in results_list if r.error == 'top_page_failed')
    mismatch = sum(1 for r in results_list if r.error == 'company_mismatch')
    no_contact = sum(1 for r in results_list if not r.error and not r.contact_url and not r.phone)

    logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚µãƒãƒªãƒ¼: ç·æ•°={total}, æˆåŠŸ={success}, ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸å¤±æ•—={top_failed}, ä¼æ¥­åä¸ä¸€è‡´={mismatch}, é€£çµ¡å…ˆæœªæ¤œå‡º={no_contact}")

    return results_list
